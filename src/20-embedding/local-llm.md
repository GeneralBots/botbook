# Local LLM with llama.cpp
